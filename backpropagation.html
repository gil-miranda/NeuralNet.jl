<!DOCTYPE html>
<HTML lang = "en">
<HEAD>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Backpropagation</title>
  

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
  </script>

  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

  
<style>
pre.hljl {
    border: 1px solid #ccc;
    margin: 5px;
    padding: 5px;
    overflow-x: auto;
    color: rgb(68,68,68); background-color: rgb(251,251,251); }
pre.hljl > span.hljl-t { }
pre.hljl > span.hljl-w { }
pre.hljl > span.hljl-e { }
pre.hljl > span.hljl-eB { }
pre.hljl > span.hljl-o { }
pre.hljl > span.hljl-k { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kc { color: rgb(59,151,46); font-style: italic; }
pre.hljl > span.hljl-kd { color: rgb(214,102,97); font-style: italic; }
pre.hljl > span.hljl-kn { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kp { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kr { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kt { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-n { }
pre.hljl > span.hljl-na { }
pre.hljl > span.hljl-nb { }
pre.hljl > span.hljl-nbp { }
pre.hljl > span.hljl-nc { }
pre.hljl > span.hljl-ncB { }
pre.hljl > span.hljl-nd { color: rgb(214,102,97); }
pre.hljl > span.hljl-ne { }
pre.hljl > span.hljl-neB { }
pre.hljl > span.hljl-nf { color: rgb(66,102,213); }
pre.hljl > span.hljl-nfm { color: rgb(66,102,213); }
pre.hljl > span.hljl-np { }
pre.hljl > span.hljl-nl { }
pre.hljl > span.hljl-nn { }
pre.hljl > span.hljl-no { }
pre.hljl > span.hljl-nt { }
pre.hljl > span.hljl-nv { }
pre.hljl > span.hljl-nvc { }
pre.hljl > span.hljl-nvg { }
pre.hljl > span.hljl-nvi { }
pre.hljl > span.hljl-nvm { }
pre.hljl > span.hljl-l { }
pre.hljl > span.hljl-ld { color: rgb(148,91,176); font-style: italic; }
pre.hljl > span.hljl-s { color: rgb(201,61,57); }
pre.hljl > span.hljl-sa { color: rgb(201,61,57); }
pre.hljl > span.hljl-sb { color: rgb(201,61,57); }
pre.hljl > span.hljl-sc { color: rgb(201,61,57); }
pre.hljl > span.hljl-sd { color: rgb(201,61,57); }
pre.hljl > span.hljl-sdB { color: rgb(201,61,57); }
pre.hljl > span.hljl-sdC { color: rgb(201,61,57); }
pre.hljl > span.hljl-se { color: rgb(59,151,46); }
pre.hljl > span.hljl-sh { color: rgb(201,61,57); }
pre.hljl > span.hljl-si { }
pre.hljl > span.hljl-so { color: rgb(201,61,57); }
pre.hljl > span.hljl-sr { color: rgb(201,61,57); }
pre.hljl > span.hljl-ss { color: rgb(201,61,57); }
pre.hljl > span.hljl-ssB { color: rgb(201,61,57); }
pre.hljl > span.hljl-nB { color: rgb(59,151,46); }
pre.hljl > span.hljl-nbB { color: rgb(59,151,46); }
pre.hljl > span.hljl-nfB { color: rgb(59,151,46); }
pre.hljl > span.hljl-nh { color: rgb(59,151,46); }
pre.hljl > span.hljl-ni { color: rgb(59,151,46); }
pre.hljl > span.hljl-nil { color: rgb(59,151,46); }
pre.hljl > span.hljl-noB { color: rgb(59,151,46); }
pre.hljl > span.hljl-oB { color: rgb(102,102,102); font-weight: bold; }
pre.hljl > span.hljl-ow { color: rgb(102,102,102); font-weight: bold; }
pre.hljl > span.hljl-p { }
pre.hljl > span.hljl-c { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-ch { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cm { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cp { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cpB { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cs { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-csB { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-g { }
pre.hljl > span.hljl-gd { }
pre.hljl > span.hljl-ge { }
pre.hljl > span.hljl-geB { }
pre.hljl > span.hljl-gh { }
pre.hljl > span.hljl-gi { }
pre.hljl > span.hljl-go { }
pre.hljl > span.hljl-gp { }
pre.hljl > span.hljl-gs { }
pre.hljl > span.hljl-gsB { }
pre.hljl > span.hljl-gt { }
</style>



  <style type="text/css">
  @font-face {
  font-style: normal;
  font-weight: 300;
}
@font-face {
  font-style: normal;
  font-weight: 400;
}
@font-face {
  font-style: normal;
  font-weight: 600;
}
html {
  font-family: sans-serif; /* 1 */
  -ms-text-size-adjust: 100%; /* 2 */
  -webkit-text-size-adjust: 100%; /* 2 */
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block; /* 1 */
  vertical-align: baseline; /* 2 */
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit; /* 1 */
  font: inherit; /* 2 */
  margin: 0; /* 3 */
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"], /* 1 */
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button; /* 2 */
  cursor: pointer; /* 3 */
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box; /* 1 */
  padding: 0; /* 2 */
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield; /* 1 */
  -moz-box-sizing: content-box;
  -webkit-box-sizing: content-box; /* 2 */
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0; /* 1 */
  padding: 0; /* 2 */
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  font-family: monospace, monospace;
  font-size : 0.8em;
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
thead th {
    border-bottom: 1px solid black;
    background-color: white;
}
tr:nth-child(odd){
  background-color: rgb(248,248,248);
}


/*
* Skeleton V2.0.4
* Copyright 2014, Dave Gamache
* www.getskeleton.com
* Free to use under the MIT license.
* http://www.opensource.org/licenses/mit-license.php
* 12/29/2014
*/
.container {
  position: relative;
  width: 100%;
  max-width: 960px;
  margin: 0 auto;
  padding: 0 20px;
  box-sizing: border-box; }
.column,
.columns {
  width: 100%;
  float: left;
  box-sizing: border-box; }
@media (min-width: 400px) {
  .container {
    width: 85%;
    padding: 0; }
}
@media (min-width: 550px) {
  .container {
    width: 80%; }
  .column,
  .columns {
    margin-left: 4%; }
  .column:first-child,
  .columns:first-child {
    margin-left: 0; }

  .one.column,
  .one.columns                    { width: 4.66666666667%; }
  .two.columns                    { width: 13.3333333333%; }
  .three.columns                  { width: 22%;            }
  .four.columns                   { width: 30.6666666667%; }
  .five.columns                   { width: 39.3333333333%; }
  .six.columns                    { width: 48%;            }
  .seven.columns                  { width: 56.6666666667%; }
  .eight.columns                  { width: 65.3333333333%; }
  .nine.columns                   { width: 74.0%;          }
  .ten.columns                    { width: 82.6666666667%; }
  .eleven.columns                 { width: 91.3333333333%; }
  .twelve.columns                 { width: 100%; margin-left: 0; }

  .one-third.column               { width: 30.6666666667%; }
  .two-thirds.column              { width: 65.3333333333%; }

  .one-half.column                { width: 48%; }

  /* Offsets */
  .offset-by-one.column,
  .offset-by-one.columns          { margin-left: 8.66666666667%; }
  .offset-by-two.column,
  .offset-by-two.columns          { margin-left: 17.3333333333%; }
  .offset-by-three.column,
  .offset-by-three.columns        { margin-left: 26%;            }
  .offset-by-four.column,
  .offset-by-four.columns         { margin-left: 34.6666666667%; }
  .offset-by-five.column,
  .offset-by-five.columns         { margin-left: 43.3333333333%; }
  .offset-by-six.column,
  .offset-by-six.columns          { margin-left: 52%;            }
  .offset-by-seven.column,
  .offset-by-seven.columns        { margin-left: 60.6666666667%; }
  .offset-by-eight.column,
  .offset-by-eight.columns        { margin-left: 69.3333333333%; }
  .offset-by-nine.column,
  .offset-by-nine.columns         { margin-left: 78.0%;          }
  .offset-by-ten.column,
  .offset-by-ten.columns          { margin-left: 86.6666666667%; }
  .offset-by-eleven.column,
  .offset-by-eleven.columns       { margin-left: 95.3333333333%; }

  .offset-by-one-third.column,
  .offset-by-one-third.columns    { margin-left: 34.6666666667%; }
  .offset-by-two-thirds.column,
  .offset-by-two-thirds.columns   { margin-left: 69.3333333333%; }

  .offset-by-one-half.column,
  .offset-by-one-half.columns     { margin-left: 52%; }

}
html {
  font-size: 62.5%; }
body {
  font-size: 1.5em; /* currently ems cause chrome bug misinterpreting rems on body element */
  line-height: 1.6;
  font-weight: 400;
  font-family: "Raleway", "HelveticaNeue", "Helvetica Neue", Helvetica, Arial, sans-serif;
  color: #222; }
h1, h2, h3, h4, h5, h6 {
  margin-top: 0;
  margin-bottom: 2rem;
  font-weight: 300; }
h1 { font-size: 3.6rem; line-height: 1.2;  letter-spacing: -.1rem;}
h2 { font-size: 3.4rem; line-height: 1.25; letter-spacing: -.1rem; }
h3 { font-size: 3.2rem; line-height: 1.3;  letter-spacing: -.1rem; }
h4 { font-size: 2.8rem; line-height: 1.35; letter-spacing: -.08rem; }
h5 { font-size: 2.4rem; line-height: 1.5;  letter-spacing: -.05rem; }
h6 { font-size: 1.5rem; line-height: 1.6;  letter-spacing: 0; }

p {
  margin-top: 0; }
a {
  color: #1EAEDB; }
a:hover {
  color: #0FA0CE; }
.button,
button,
input[type="submit"],
input[type="reset"],
input[type="button"] {
  display: inline-block;
  height: 38px;
  padding: 0 30px;
  color: #555;
  text-align: center;
  font-size: 11px;
  font-weight: 600;
  line-height: 38px;
  letter-spacing: .1rem;
  text-transform: uppercase;
  text-decoration: none;
  white-space: nowrap;
  background-color: transparent;
  border-radius: 4px;
  border: 1px solid #bbb;
  cursor: pointer;
  box-sizing: border-box; }
.button:hover,
button:hover,
input[type="submit"]:hover,
input[type="reset"]:hover,
input[type="button"]:hover,
.button:focus,
button:focus,
input[type="submit"]:focus,
input[type="reset"]:focus,
input[type="button"]:focus {
  color: #333;
  border-color: #888;
  outline: 0; }
.button.button-primary,
button.button-primary,
input[type="submit"].button-primary,
input[type="reset"].button-primary,
input[type="button"].button-primary {
  color: #FFF;
  background-color: #33C3F0;
  border-color: #33C3F0; }
.button.button-primary:hover,
button.button-primary:hover,
input[type="submit"].button-primary:hover,
input[type="reset"].button-primary:hover,
input[type="button"].button-primary:hover,
.button.button-primary:focus,
button.button-primary:focus,
input[type="submit"].button-primary:focus,
input[type="reset"].button-primary:focus,
input[type="button"].button-primary:focus {
  color: #FFF;
  background-color: #1EAEDB;
  border-color: #1EAEDB; }
input[type="email"],
input[type="number"],
input[type="search"],
input[type="text"],
input[type="tel"],
input[type="url"],
input[type="password"],
textarea,
select {
  height: 38px;
  padding: 6px 10px; /* The 6px vertically centers text on FF, ignored by Webkit */
  background-color: #fff;
  border: 1px solid #D1D1D1;
  border-radius: 4px;
  box-shadow: none;
  box-sizing: border-box; }
/* Removes awkward default styles on some inputs for iOS */
input[type="email"],
input[type="number"],
input[type="search"],
input[type="text"],
input[type="tel"],
input[type="url"],
input[type="password"],
textarea {
  -webkit-appearance: none;
     -moz-appearance: none;
          appearance: none; }
textarea {
  min-height: 65px;
  padding-top: 6px;
  padding-bottom: 6px; }
input[type="email"]:focus,
input[type="number"]:focus,
input[type="search"]:focus,
input[type="text"]:focus,
input[type="tel"]:focus,
input[type="url"]:focus,
input[type="password"]:focus,
textarea:focus,
select:focus {
  border: 1px solid #33C3F0;
  outline: 0; }
label,
legend {
  display: block;
  margin-bottom: .5rem;
  font-weight: 600; }
fieldset {
  padding: 0;
  border-width: 0; }
input[type="checkbox"],
input[type="radio"] {
  display: inline; }
label > .label-body {
  display: inline-block;
  margin-left: .5rem;
  font-weight: normal; }
ul {
  list-style: circle; }
ol {
  list-style: decimal; }
ul ul,
ul ol,
ol ol,
ol ul {
  margin: 1.5rem 0 1.5rem 3rem;
  font-size: 90%; }
li > p {margin : 0;}
th,
td {
  padding: 12px 15px;
  text-align: left;
  border-bottom: 1px solid #E1E1E1; }
th:first-child,
td:first-child {
  padding-left: 0; }
th:last-child,
td:last-child {
  padding-right: 0; }
button,
.button {
  margin-bottom: 1rem; }
input,
textarea,
select,
fieldset {
  margin-bottom: 1.5rem; }
pre,
blockquote,
dl,
figure,
table,
p,
ul,
ol,
form {
  margin-bottom: 1.0rem; }
.u-full-width {
  width: 100%;
  box-sizing: border-box; }
.u-max-full-width {
  max-width: 100%;
  box-sizing: border-box; }
.u-pull-right {
  float: right; }
.u-pull-left {
  float: left; }
hr {
  margin-top: 3rem;
  margin-bottom: 3.5rem;
  border-width: 0;
  border-top: 1px solid #E1E1E1; }
.container:after,
.row:after,
.u-cf {
  content: "";
  display: table;
  clear: both; }

pre {
  display: block;
  padding: 9.5px;
  margin: 0 0 10px;
  font-size: 13px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre.hljl {
  margin: 0 0 10px;
  display: block;
  background: #f5f5f5;
  border-radius: 4px;
  padding : 5px;
}

pre.output {
  background: #ffffff;
}

pre.code {
  background: #ffffff;
}

pre.julia-error {
  color : red
}

code,
kbd,
pre,
samp {
  font-family: Menlo, Monaco, Consolas, "Courier New", monospace;
  font-size: 0.9em;
}


@media (min-width: 400px) {}
@media (min-width: 550px) {}
@media (min-width: 750px) {}
@media (min-width: 1000px) {}
@media (min-width: 1200px) {}

h1.title {margin-top : 20px}
img {max-width : 100%}
div.title {text-align: center;}

  </style>
</HEAD>

<BODY>
  <div class ="container">
    <div class = "row">
      <div class = "col-md-12 twelve columns">
        <div class="title">
          <h1 class="title">Backpropagation</h1>
          <h5>Gil Miranda</h5>
          <h5>19 de Janeiro de 2021</h5>
        </div>

        <h1>Um algoritmo para treinar a rede neural</h1>
<p>Queremos que o output da rede neural <span class="math">$NN(x)$</span> seja o mais próximo possível dos nossos dados, isto é, estamos aproximando uma função <span class="math">$f$</span> a qual não conhecemos explicitamente, mas apenas implicitamente atráves de alguns exemplos &#40;dados de treino&#41;. Denotando <span class="math">$y(x)$</span> o output real de um dado de entrada <span class="math">$x$</span>, e <span class="math">$a(x)$</span> o output da rede neural, aproximar as funções pode ser visto como manter a diferença quadrática entre elas pequena, daí a função custo, definida como</p>
<p class="math">\[
C(w,b) = \frac{1}{2n}\sum_j || y(x_j) - a(w,b,x_j) ||^2_2
\]</p>
<h3>Relação entre função custo e o Backpropagation</h3>
<p>A função custo é uma função dos pesos &#40;<span class="math">$w$</span>&#41; e vieses &#40;<span class="math">$b$</span>&#41;.</p>
<p>Minimizar a função custo somada em todos os exemplos de treino é termos cada exemplo o mais próximo possível da saída da rede neural. Minimizamos a função custo atráves da técnica de <code>Stochastic Gradient Descent</code> &#40;Gradiente descendente estocástico&#41;, que consiste em seguir um &quot;caminho&quot; pela função custo para o seu minimo global, ou um bom minímo local. Calcular o Gradiente de <span class="math">$C(w,b)$</span> dado por <span class="math">$\nabla C(w,b) = (\frac{\partial C}{\partial w}, \frac{\partial C}{\partial b})$</span> pode ser bem complicado, e em uma rede neural com muitas camadas e muitos dados de treino, fica ainda mais complicado e custoso, aqui que entra o algoritimo conhecido como <code>Backpropagation</code>, é o algoritmo que auxilia no cálculo do Gradiente da função custo de uma forma menos custosa. Então <code>Backpropagation</code> não faz necessariamente o &#39;aprendizado&#39; da rede neural, isto é feito pelo algoritimo de <code>Gradient Descent</code>, <code>Backpropagation</code> é apenas a ferramenta pela qual calculamos este gradiente.</p>
<h3>Stochastic Gradient Descent</h3>
<p><code>Gradient Descent</code> é uma maneira de tomarmos um pequeno passo na direção de decrescer a função <span class="math">$C(w,b)$</span> o máximo possível. Seja <span class="math">$\theta$</span> o vetor de parâmetros da rede neural. Podemos pensar na função custo como <span class="math">$C(\theta)$</span>, se tomarmos</p>
<p class="math">\[
\Delta \theta = (\Delta \theta_1, \Delta \theta_2)
\]</p>
<p>Podemos escrever</p>
<p class="math">\[
\Delta C \approx \nabla C\cdot \Delta \theta = \frac{\partial C}{\partial \theta_1}\Delta \theta_1 + \frac{\partial C}{\partial \theta_2}\Delta \theta_2
\]</p>
<p>Tomando <span class="math">$\Delta \theta = -\eta \nabla C$</span>, <span class="math">$\eta > 0$</span>, temos</p>
<p class="math">\[
\Delta C \approx -\eta \nabla C \cdot \nabla C = -\eta ||\nabla C||^2 \leq 0
\]</p>
<p>Assim garantimos que estamos andando em uma direção não-crescente da função custo, com uma esperança de atingir o minímo global.</p>
<p>A ideia por trás do <code>Gradiente Descendente Estocástico</code> é a de tomar uma amostra menor do conjunto de dados para treino, tomando essa amostra de forma aleatória, com <span class="math">$m < n$</span> podemos ter</p>
<p class="math">\[
\frac{1}{m}\sum_{j=1}^m C_{X_j} \approx \frac{1}{n}\sum_{x} C_x = \nabla C
\]</p>
<hr />
<h1>Calculando derivadas, o algoritmo de Backpropagation</h1>
<p>Vamos olhar um diagrama de como funciona o calculo da derivada de uma composição de funções utilizando <code>Backpropagation</code></p>
<p><img src="feedforward.png" alt="Feedforward" /></p>
<p>Figura 1. Passo do <code>Feedforward</code></p>
<p><img src="backprop.png" alt="Backpropagation" /></p>
<p>Figura 2. Passo do <code>Backpropagation</code></p>
<p>Figuras retiradas de <code>&#40;2&#41; Rojas, Raul. Neural Networks</code></p>
<h2>Backpropagation em ação</h2>
<p>Vamos introduzir uma notação para incorporar as camadas da rede neural nas equações, seja <span class="math">$w^l_{jk}$</span> o peso que vai do neuron <span class="math">$k$</span> da camada <span class="math">$l-1$</span> para o neuron <span class="math">$j$</span> na camada <span class="math">$l$</span>, <span class="math">$w^l$</span> é a matriz de pesos da camada <span class="math">$l$</span>. Da mesma maneira, <span class="math">$b^l_j$</span> é o vies do neuron <span class="math">$j$</span> na camada <span class="math">$l$</span>, e <span class="math">$b^l$</span> todos os vieses da camada <span class="math">$l$</span>. <span class="math">$a^l_j$</span> é a ativação, output da função <span class="math">$\sigma$</span> no neuron <span class="math">$j$</span> na camada <span class="math">$l$</span>, <span class="math">$a^l$</span> é o vetor de ativações da camada <span class="math">$l$</span> e <span class="math">$a^L$</span> é o output final da rede neural.</p>
<p class="math">\[
a^l_j = \sigma\left(\sum_k w^l_{jk} a^{l-1}_k + b^l_j\right)
\]</p>
<p>Para o cálculo do gradiente, precisamos calcular as quantias <span class="math">$\frac{\partial C}{\partial w^l_{jk}}$</span>  e <span class="math">$\frac{\partial C}{\partial b^l_j}$</span>, definindo o erro no neuron <span class="math">$j$</span> na camada <span class="math">$l$</span> como</p>
<p class="math">\[
\delta_j^l := \frac{\partial C}{\partial z_j^l}
\]</p>
<p>Onde <span class="math">$z^l_j = \sum_k w^l_{jk} a^{l-1}_k + b^l_j$</span>. Com backpropagation conseguiremos calcular este erro e relacionar às derivadas que  queremos encontrar, vamos olhar como calcula-las</p>
<h4>Erro na camada de saída</h4>
<p>O erro na camada de saída da rede neural é dado calculando <span class="math">$\delta^L_j$</span> pela definição acima</p>
<p class="math">\[
\begin{align*}
\delta_j^L &= \frac{\partial C}{\partial z^L_j}\\
&= \sum_k \frac{\partial C}{\partial a^L_k} \frac{\partial a^L_k}{\partial z^L_j}\\
&= \frac{\partial C}{\partial a^L_j} \frac{\partial a^L_j}{\partial z^L_j}\\
\therefore \, \delta_j^L &= \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j)
\end{align*}
\]</p>
<p>O somatório na segunda linha desaparece pois a derivada <span class="math">$\frac{\partial a^L_k}{\partial z^L_j}$</span> só é não nula quando <span class="math">$k = j$</span></p>
<p>Podemos reescrever a equação de forma matricial utilizando multiplicação usual de matrizes</p>
<p class="math">\[
\begin{align}
\delta^L = \Sigma'(z^L)\nabla_a C
\end{align}
\]</p>
<p>onde <span class="math">$\Sigma'(z^L)$</span> é uma matriz diagonal com as entradas sendo <span class="math">$\sigma'(z^L_j)$</span> Ou também com um <code>produto de Hadamard</code></p>
<p class="math">\[
\delta^L = \nabla_a C \odot \sigma'(z^L)
\]</p>
<h4>Erro na camada <span class="math">$l$</span> em função do erro na camada <span class="math">$l+1$</span></h4>
<p>Vamos ver como calcular o erro em uma camada qualquer da rede neural, para isso escreveremos o erro em função do erro da camada a frente da camada de interesse.</p>
<p class="math">\[
\begin{align*}
\delta_j^l &= \frac{\partial C}{\partial z^l_j}\\
&= \sum_k \frac{\partial C}{\partial z^{l+1}_k} \frac{\partial z^{l+1}_k}{\partial z^l_j}\\
&= \sum_k \frac{\partial z^{l+1}}{\partial z_j^l} \delta_k^{l+1}\\
\therefore \delta_j^l &= \sum_k w^{l+1}_{kj}\delta^{l+1}_k\sigma'(z^l_j)
\end{align*}
\]</p>
<p>A passagem da segunda para a terceira linha apenas aplicamos a definição de <span class="math">$\delta^{l+1}_j = \partial C/\partial z^{l+1}_j$</span></p>
<p>Da terceira para a quarta linha a justificativa segue nas contas abaixo</p>
<p class="math">\[
\begin{align*}
z^{l+1}_k &= \sum_j w^{l+1}_{kj}\sigma(z^l) + b^{l+1}_k\\
\therefore \frac{\partial z^{l+1}_k}{z^{1}_k} &= w^{l+1}_{kj} \sigma'(z^l_j)
\end{align*}
\]</p>
<p>Podemos reescrever de forma matricial com o produto usual</p>
<p class="math">\[
\begin{align}
\delta^l = \Sigma'(z^l)(w^{l+1})^T\delta^{l+1}
\end{align}
\]</p>
<p>Ou com produto de Hadamard</p>
<p class="math">\[
\delta^l = ((w^{l+1})^T\delta^{l+1})\odot \sigma'(z^l)
\]</p>
<p>Com essas duas equações sabemos calcular o erro em qualquer camada <span class="math">$l$</span> da rede neural dado o erro da camada <span class="math">$l+1$</span>, e sabemos calcular o erro na camada de saída, portanto conseguimos calcular todos de modo iterativo para trás. Falta agora relacionar os erros <span class="math">$\delta$</span> com as quantias de interesse que são as derivadas parciais da função custo em relação aos parâmetros da rede.</p>
<h4>Derivada da função custo em relação aos vieses</h4>
<p>Queremos uma expressão que relacione <span class="math">$\partial C/\partial b^l$</span> com <span class="math">$\delta^l$</span>, vamos então aplicar a regra da cadeia na definição de <span class="math">$\delta^l$</span></p>
<p class="math">\[
\begin{align*}
\delta^l_j &= \sum_k \frac{\partial C}{\partial b^l_k}\frac{\partial b^l_k}{\partial z^l_j}\\
&= \frac{\partial C}{\partial b^l_j}\frac{\partial b^l_j}{\partial z^l_j}\\
&\text{Usando a definição de $z^l_j$ e derivando}\\
z^l_j &= \sum_k w^l_{kj}a_k^{l-1} + b^l_j\\
b^l_j &= z^l_j -  \sum_k w^l_{kj}a_k^{l-1}\\
\frac{\partial b^l_j}{\partial z^l_j} &= 1\\
\end{align*}
\]</p>
<p class="math">\[
\begin{align}
\therefore \, \frac{\partial C}{\partial b^l_j} = \delta^l_j
\end{align}
\]</p>
<p>Desta forma já temos calculado uma das derivadas de interesse.</p>
<h4>Derivada da função custo em relação aos pesos</h4>
<p>Agora queremos uma expressão para <span class="math">$\partial C/\partial w_{jk}^l$</span></p>
<p class="math">\[
\begin{align*}
\frac{\partial C}{\partial w^l_{jk}} &= \frac{\partial C}{\partial z^l_j} \frac{\partial z^l_j}{\partial w^l_{jk}}\\
&= \delta^l_j \frac{\partial z^l_j}{\partial w^l_{jk}}\\
&= \delta^l_j a^{l-1}_k\\
\end{align*}
\]</p>
<p class="math">\[
\begin{align}
\therefore \, \frac{\partial C}{\partial w^l_{jk}} =  a^{l-1}_k\delta^l_j
\end{align}
\]</p>
<p>Com isso estamos prontos para calcular todas as derivadas de interesse para o <code>Stochastic Gradient Descent</code> utilizando <code>Backpropagation</code>.</p>
<h3>Resumo do algoritmo para treinamento de uma rede neural</h3>
<ol>
<li><p>Enviar o dado de entrada pela rede atráves de Feedforward</p>
</li>
<li><p>Backpropagation na camada de saída</p>
<ul>
<li><p>Utilizando a equação <span class="math">$(1)$</span>, calculamos o erro na saída da rede neural</p>
</li>
</ul>
</li>
<li><p>Backpropagation nas camadas ocultas</p>
<ul>
<li><p>Utilizando a equação <span class="math">$(2)$</span>, calculamos o erro em cada camada e utilizando as equações <span class="math">$(3), (4)$</span>, calculamos as derivadas parciais da função custo em relação aos parâmetros</p>
</li>
</ul>
</li>
<li><p>Atualizar os pesos e vieses da rede</p>
<ul>
<li><p>Utilizando o gradiente calculado no passo anterior, damos um passo na direção de um minímo da função custo, atualizando os pesos e vieses.</p>
</li>
</ul>
</li>
<li><p>O algoritmo termina a execução quando atingimos um valor suficientemente pequeno da função custo <span class="math">$C(w,b)$</span></p>
</li>
</ol>
<p>O algoritmo quando implementado numericamente, utiliza o <code>Produto de Hadamard</code> nas equações <span class="math">$(1), (2)$</span> ao invés do produto usual de matrizes pois é menos custoso computacionalmente</p>
<hr />
<h3>Implementando do zero uma rede neural em Julia</h3>
<h1>Bibliografia</h1>
<ol>
<li><p><a href="https://www.deeplearningbook.org/">Goodfellow, Ian  &#40;2016&#41;. Deep Learning, MIT Press</a></p>
</li>
<li><p><a href="https://page.mi.fu-berlin.de/rojas/neural/">Rojas, Raul &#40;1996&#41;. Neural Networks - A Systematic Introduction, Springer</a></p>
</li>
<li><p><a href="http://neuralnetworksanddeeplearning.com">Michael A. Nielsen &#40;2015&#41;, Neural Networks and Deep Learning, Determination Press</a></p>
</li>
</ol>


        <HR/>
        <div class="footer">
          <p>
            Published from <a href="backpropagation.jmd">backpropagation.jmd</a>
            using <a href="http://github.com/JunoLab/Weave.jl">Weave.jl</a> v0.10.6 on 2021-02-16.
          </p>
        </div>
      </div>
    </div>
  </div>
</BODY>

</HTML>
